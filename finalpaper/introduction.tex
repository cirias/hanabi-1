\section{Introduction}\label{sec:intro}

In this paper, we present a case study on the feasibility of using machine
learning to perform a cooperative task in which the combined agents have
complete information, but each individual agent has only partial information.
We train an AI to learn to play Hanabi: a cooperative card game in which each
player can see everyone else's cards, but not their own~\cite{hanabiboardgame,
hanabiwiki}.

The cooperative aspect of Hanabi differentiates it from traditional card games,
in which adversaries have inverse objectives. In Hanabi, the basic moves
include playing a card, which may return some points for the group, or giving
information to another agent, which may enable them to play a card on a later
move. This means that points can often only be gained through a cooperative
strategy. We discuss Hanabi gameplay and strategy in more detail in
\secref{background:hanabi}.

A natural fit for learning to play Hanabi is to frame Hanabi as a Markov
decision process (MDP)~\cite{mdp}. MDPs model problems that can be encoded as a
state machine, in which each state transition is partly stochastic and partly
influenced by an agent's action. There are various \emph{reinforcement learning
algorithms}~\cite{rubinstein2013cross, policyoptimizationNIPS2016, cmaes, TRPO}
that can solve an MDP to produce a \emph{policy} that decides what action to
take in a given state in order to obtain the biggest \emph{reward}. We discuss
MDPs and reinforcement learning in \secref{background:mdp}.

The primary challenge is then to encode Hanabi as an MDP. In
\secref{hanabimdp}, we explore various methods of representing state and reward
to facilitate learning. Representing state properly is challenging because each
agent has only partial information about the entire game state, so some actions
may be directly dependent on some unobserved state.

Using these Hanabi encodings, we then apply reinforcement learning to train
policies that can play multiple variants of Hanabi. In \secref{learninghanabi},
we discuss the methods used to train the Hanabi AIs, the results of which we
present in \secref{eval}.  On a reduced variant of Hanabi, our best policy
achieves a perfect score on 9\% of the games it plays and achieves an average
score of 77\%.
