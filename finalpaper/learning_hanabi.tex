\section{Learning to Play Hanabi}\label{sec:learninghanabi}

In this section, we describe how to apply the TRPO, CEM, and CMA-ES
reinforcement learning algorithms to a Hanabi MDP.

\subsection{Learning Methods}
The reinforcement learning algorithms we consider begin with a random policy
and continuously improve the policy by interacting with the environment.
Because Hanabi is a two-player game, the effect that one player's action has on
the environment depends critically on the policy of the other player. Thus,
when we apply a reinforcement learning algorithm to learn one player's policy,
we have to carefully choose a policy for the other player.

One option is to learn the policy against a hand-written Hanabi policy that
plays based on some predetermined heuristics. We call this approach
\emph{guided-learning}. Guided learning allows us to codify our domain-specific
knowledge about Hanabi to help guide the learned policy. However, the
effectiveness of a policy learned with guided-learning is limited by the
effectiveness of the heuristic-based policy. For example, training against a
completely random heuristic-based policy expectedly produces policies that
perform poorly.

Another option is to play the policy against itself: a learning method we call
\emph{dynamic self-learning}. With dynamic self-learning, a single policy acts
on behalf of both players, and a reinforcement learning algorithm aims to
learn this single policy. The performance of a dynamically self-learned
policy is not limited by a hand-written policy. On the other hand, when an
algorithm begins with a random policy, the policy is forced to play against an
ineffective policy (i.e.\ itself).

With dynamic self-learning, the behavior of the environment depends on the
policy, so the environment is repeatedly changed as the policy is improved. In
effect, this creates a moving target for reinforcement learning algorithms. An
alternative to dynamic self-learning is \emph{static self-learning} in which a
policy is played against a fixed previous snapshot of itself. Every $k$
iterations ($k$ is a hyperparameter), an algorithm creates a checkpoint of the
policy and uses it to train the policy for the next $k$ iterations. This
approach is not limited by a hand-written policy and avoids rapidly changing
the environment.

\subsection{Heuristics}

In this section we describe the two heuristics that we use for guided learning,
\texttt{ComplexHeuristic} and \texttt{SimpleHeuristic}.

\subsubsection{\texttt{ComplexHeuristic}}

The \texttt{ComplexHeuristic} player mimics a human's strategy for mini-Hanabi. On each
turn, the player computes cards that can be played based on rank information
and the highest ranks of played so far. The player computes a card that it will
discard, if necessary; this is the card for which it has the least information.
Finally, the player computes useful information based on what cards it thinks
the other player will play or discard. This may include cards that are
playable, cards that will result in a fuse if played, etc.

\subsubsection{\texttt{SimpleHeuristic}}

\todo{can save time by moving this to the learned policy section?  - Sagar}

The \texttt{SimpleHeuristic} player mimics the strategy that our best
TRPO-trained-on-itself player learns for mini-hanabi.  Each turn, it iterates
over the card numbers, starting at one and tries the following:

\begin{itemize}[leftmargin=*]
\item Give info about cards of the given number if there are any that the other player does not know about
\item If a card of the given number is already played for each color, discard any card of that number that we know about in our hand
\item Play a card of the given number.
\end{itemize}

If none of these conditions are satisfied, the player tries to play the zeroeth card in
its hand. We avoid discarding as much as possible, since every discard after
the first two costs one point in two-player mini-hanabi.

\subsection{Algorithms}
We experimented with three policy optimization algorithms provided in
\texttt{rllab}. Below, we briefly describe these algorithms:

\begin{itemize}
    \item Cross-Entropy Method (CEM)~\cite{rubinstein2013cross,
        policyoptimizationNIPS2016}: This method maintains a population of
        policies (i.e.\ parameter vectors) and a probability distribution (such
        as a Gaussian) with some distribution parameter.  Each iteration, it
        samples parameter vectors from the distribution and executes rollouts
        using the sampled policy, storing the obtained utility.  The top $n$
        percent of policies with the highest provided utility are used to fit
        a new distribution parameter for the next timestep. In the CEM case,
        this fit is performed by maximizing the log probability of obtaining
        the top $n$ percent of the parameter vectors. The process then repeats
        for the supplied number of iterations. This essentially optimizes by 
        treating the utility as a black box.

    \item Covariance Matrix Adaptation Evolutionary Strategy
        (CMA-ES)~\cite{cmaes, policyoptimizationNIPS2016}: This method is
        similar to CEM, except that it replaces the distribution with
        a multivariate Gaussian, and fits to obtain a mean and covariance matrix.
        This method also attempts to produce a result each iteration that is
        close to the previous result, as opposed to CEM, which always takes the
        latest rollout results.

    \item Trust Region Policy Optimization (TRPO)~\cite{TRPO,
        policyoptimizationNIPS2016}: This method iteratively optimizes policies
        in a similar fashion as natural policy gradient methods. It optimizes
        a surrogate function that approximates the expected return of a policy.
        In contrast to the previous methods, TRPO is better-suited for
        optimizing large numbers of parameters, such as those for neural
        network based policies.
\end{itemize}
