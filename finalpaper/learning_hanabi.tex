\section{Learning to Play Hanabi}\label{sec:learninghanabi}

In this section, we describe how to apply specific reinforcement learning
algorithms and training methods to solve a Hanabi MDP.

\subsection{Learning Methods}
The reinforcement learning algorithms we consider begin with a random policy
and continuously improve the policy by interacting with the environment.
Because Hanabi is a two-player game, the effect that one player's action has on
the environment depends critically on the policy of the other player. Thus,
when we apply a reinforcement learning algorithm to learn one player's policy,
we have to carefully choose a policy for the other player.

One option is to learn the policy against a hand-written Hanabi policy that
plays based on some predetermined heuristics. We call this approach
\emph{guided-learning}. Guided learning allows us to codify our domain-specific
knowledge about Hanabi to help guide the learned policy. However, the
effectiveness of a policy learned with guided-learning is limited by the
effectiveness of the heuristic-based policy. For example, training against a
completely random heuristic-based policy expectedly produces policies that
perform poorly.

Another option is to play the policy against itself: a learning method we call
\emph{dynamic self-learning}. With dynamic self-learning, a single policy acts
on behalf of both players, and a reinforcement learning algorithm aims to
learn this single policy. The performance of a dynamically self-learned
policy is not limited by a hand-written policy. On the other hand, when an
algorithm begins with a random policy, the policy is forced to play against an
ineffective policy (i.e.\ itself).

With dynamic self-learning, the behavior of the environment depends on the
policy, so the environment is repeatedly changed as the policy is improved. In
effect, this creates a moving target for reinforcement learning algorithms. An
alternative to dynamic self-learning is \emph{static self-learning} in which a
policy is played against a fixed previous snapshot of itself. Every $k$
iterations ($k$ is a hyperparameter), an algorithm creates a checkpoint of the
policy and uses it to train the policy for the next $k$ iterations. This
approach is not limited by a hand-written policy and avoids rapidly changing
the environment.

\subsection{Heuristic}

In this section we describe the heuristic that we use for guided learning.  The
heuristic player mimics a human's strategy for Hanabi. On each turn, the
heuristic computes cards that can be played based on rank information and the
highest ranks played so far. The heuristic computes which card it will
discard, if necessary; this is the card for which it has the least information.
Finally, the heuristic computes useful information to give to the other player
based on what cards it thinks the other player will play or discard. This may
include cards that are playable, cards that will result in a fuse if played,
etc.

\subsection{Algorithms}
We experimented with three policy optimization algorithms provided by the
\texttt{rllab} reinforcement learning library~\cite{duan2016benchmarking}.
Below, we briefly describe these algorithms:

\paragraph{Cross-Entropy Method (CEM)~\cite{rubinstein2013cross,
policyoptimizationNIPS2016}}
This method maintains a population of policies (i.e.\ parameter vectors) and a
probability distribution (such as a Gaussian) with some distribution parameter.
Each iteration, it samples parameter vectors from the distribution and executes
rollouts using the sampled policy, storing the obtained utility.  The top $n$
percent of policies with the highest provided utility are used to fit a new
distribution parameter for the next timestep. In the CEM case, this fit is
performed by maximizing the log probability of obtaining the top $n$ percent of
the parameter vectors. The process then repeats for the supplied number of
iterations. This essentially optimizes by treating the utility as a black box.

\paragraph{Covariance Matrix Adaptation Evolutionary Strategy
(CMA-ES)~\cite{cmaes, policyoptimizationNIPS2016}}
This method is similar to CEM, except that it replaces the distribution with a
multivariate Gaussian, and fits to obtain a mean and covariance matrix.  This
method also attempts to produce a result each iteration that is close to the
previous result, as opposed to CEM, which always takes the latest rollout
results.

\paragraph{Trust Region Policy Optimization (TRPO)~\cite{TRPO,
policyoptimizationNIPS2016}}
This method iteratively optimizes policies in a similar fashion as natural
policy gradient methods. It optimizes a surrogate function that approximates
the expected return of a policy.  In contrast to the previous methods, TRPO is
better-suited for optimizing large numbers of parameters, such as those for
neural network based policies.
