\section{Evaluation}\label{sec:eval}

We trained multiple reinforcement learning algorithms on multiple variants of
Hanabi using dynamic self-learning, guided-learning, and static self-learning.
In this section, we answer questions about how well a learned policy can play
Hanabi, about how self-learning differs from guided-learning, and about how
learned policies qualitatively play Hanabi.

\subsection{Hanabi Variants}
We consider three variants of Hanabi. The first, which we call \emph{Hanabi},
is a standard game of Hanabi. The second, \emph{medium Hanabi}, is Hanabi with
only four colors and four numbers (three 1's, two 2's, two 3's, one 4). The
third, \emph{mini Hanabi}, is Hanabi with three colors, three numbers (two 1's,
two 2's, one 3), six information tokens, and two fuses.

\subsection{Hyperparameter Tuning and Algorithm Selection}
We consider three reinforcement learning algorithms: TRPO, CEM, and CMA-ES. In
order to compare the three algorithms fairly, we first tune the hyperparameters
of each algorithm. For example, \figref{trpo-tuning} shows the average reward
of a TRPO dynamically self-learned mini Hanabi policy against the number of
training iterations for various values of hyperparameters.

\begin{figure}[ht]
  \newcommand{\hyperparamsubfig}[3]{%
    \begin{subfigure}[t]{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{data/sweep/reward_vs_iteration_#1.pdf}
      \caption{#2}\label{fig:#3}
    \end{subfigure}
  }

  \centering

  \hyperparamsubfig{hidden_sizes}{the number and size of hidden layers}{}
  \hyperparamsubfig{batch_size}{batch size}{}
  \hyperparamsubfig{discount}{discount factor}{}

  \hyperparamsubfig{step_size}{step size}{}
  \hyperparamsubfig{reward}{reward function}{}
  \hyperparamsubfig{space}{state space}{}

  \caption{
    Hyperparameter tuning for TRPO trained mini Hanabi policies. The default
    values for each hyperparameter (if not being varied) are as follows: hidden
    sizes=$(16, 16)$, discount=$1$, step size=$0.01$, reward function=constant,
    space=nested.
  }\label{fig:trpo-tuning}
\end{figure}

We then train a dynamically self-learned mini Hanabi policy using each
algorithm for 1000 training iterations. Histograms of the policies' scores on
1000 games of mini Hanabi are shown in \figref{algos}.

The CMA-ES performs very poorly with an average score of 1.757.  It performs
only marginally better than a completely random policy which obtains an average
score of 1.56.
%
The CEM policy performs slightly better (but still poorly) with an average
score of 2.215.
%
The TRPO policy performs significantly better than either of the other two
policies. Most games result in a score of 8, and a significant number of games
result in a perfect score.
%
We conduct the rest of our experiments with TRPO.

\begin{figure}[ht]
  \newcommand{\algosubfig}[3]{%
    \begin{subfigure}[b]{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{data/sweep2/#1.pdf}
      \caption{#2}\label{fig:#3}
    \end{subfigure}
  }

  \centering
  \algosubfig{CMA-ES}{CMA-ES}{cmaes}
  \algosubfig{CEM}{CEM}{cem}
  \algosubfig{TRPO}{TRPO}{trpo}
  \caption{Mini Hanabi scores on 1000 mini Hanabi games.}\label{fig:algos}
\end{figure}

\subsection{Self Learning vs Guided Learning}
We train a TRPO policy for 2000 iterations using dynamic self-learning, guided
learning, and fixed self-learning for mini Hanabi, medium Hanabi, and Hanabi.
We then play each policy against itself on 1000 games and plot a histogram of
the scores in \figref{best}. For comparison, we also show the scores for our
heuristic when played against itself.

For mini Hanabi, all three learned policies outperform the hand-written
heuristic. Moreover, both self-learned policies outperform the guided-learned
policy. This confirms our hypothesis that performance of a guided-learned
policy is bounded by the performance of the heuristic against which it is
trained.  Moreover, the dynamic self-learned policy achieves a significant
number of perfect scores and averages 74\% of a perfect score.

For medium Hanabi and Hanabi, all three learned policies perform worse than the
heuristic. We conjecture that the larger state space of medium Hanabi and
Hanabi require more training time to achieve comparable performance to the
policies trained on mini Hanabi which has a considerably smaller state space.
Moreover, both self-learned policies perform worse than the guided-learned
policy which confirms our hypothesis that guided-learned policies can more
quickly improve while self-learned policies initially struggle to improve. We
are confident that given enough training time, the learned policies would
outperform the heuristic-based policy.

\begin{figure}[ht]
  \newcommand{\bestsubfig}[1]{%
    \begin{subfigure}[b]{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{data/sweep_best/#1.pdf}
    \end{subfigure}
  }

  \centering

  \makebox[\textwidth][c]{%
    \begin{tabular}{|c|c|c|c|}
      \cline{2-4}
      \multicolumn{1}{c}{} &
      \multicolumn{1}{|c|}{\textbf{Mini Hanabi}} &
      \textbf{Medium Hanabi} &
      \textbf{Hanabi} \\\hline
      %
      \rotatebox[origin=l]{90}{\textbf{Dynamic Self-Learning}} &
      \bestsubfig{mini_Hanabi_dynamic_self-learned} &
      \bestsubfig{medium_Hanabi_dynamic_self-learned} &
      \bestsubfig{Hanabi_dynamic_self-learned} \\\hline
      %
      \rotatebox[origin=l]{90}{\textbf{Guided-Learning}} &
      \bestsubfig{mini_Hanabi_guided-learned} &
      \bestsubfig{medium_Hanabi_guided-learned} &
      \bestsubfig{Hanabi_guided-learned} \\\hline
      %
      \rotatebox[origin=l]{90}{\textbf{Static Self-Learning}} &
      \bestsubfig{mini_Hanabi_static_self-learned} &
      \bestsubfig{medium_Hanabi_static_self-learned} &
      \bestsubfig{Hanabi_static_self-learned} \\\hline
      %
      \rotatebox[origin=l]{90}{\textbf{Heuristic}} &
      \bestsubfig{mini_Hanabi_heuristic} &
      \bestsubfig{medium_Hanabi_heuristic} &
      \bestsubfig{Hanabi_heuristic} \\\hline
    \end{tabular}
  }

  \caption{}\label{fig:best}
\end{figure}
