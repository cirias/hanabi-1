\section{Evaluation}\label{sec:eval}

We trained multiple reinforcement learning algorithms on multiple variants of
Hanabi using dynamic self-learning, guided-learning, and static self-learning.
In this section, we answer questions about how well a learned policy can play
Hanabi, about how self-learning differs from guided-learning, and about how
learned policies qualitatively play Hanabi.

\subsection{Hanabi Variants}
We consider three variants of Hanabi. The first, which we call \emph{Hanabi},
is a standard game of Hanabi. The second, \emph{medium Hanabi}, is Hanabi with
only four colors and four numbers (three 1's, two 2's, two 3's, one 4). The
third, \emph{mini Hanabi}, is Hanabi with three colors, three numbers (two 1's,
two 2's, one 3), six information tokens, and two fuses.

\subsection{Hyperparameter Tuning and Algorithm Selection}
We consider three reinforcement learning algorithms: TRPO, CEM, and CMA-ES. In
order to compare the three algorithms fairly, we first tune the hyperparameters
of each algorithm. For example, \figref{trpo-tuning} shows the average reward
of a TRPO dynamically self-learned mini Hanabi policy against the number of
training iterations for various values of hyperparameters.

\begin{figure}[ht]
  \newcommand{\hyperparamsubfig}[3]{%
    \begin{subfigure}[t]{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{data/sweep/reward_vs_iteration_#1.pdf}
      \caption{#2}\label{fig:#3}
    \end{subfigure}
  }

  \centering

  \hyperparamsubfig{hidden_sizes}{the number and size of hidden layers}{}
  \hyperparamsubfig{batch_size}{batch size}{}
  \hyperparamsubfig{discount}{discount factor}{}

  \hyperparamsubfig{step_size}{step size}{}
  \hyperparamsubfig{reward}{reward function}{}
  \hyperparamsubfig{space}{state space}{}

  \caption{
    Hyperparameter tuning for TRPO trained mini Hanabi policies. The default
    values for each hyperparameter (if not being varied) are as follows: hidden
    sizes=$(16, 16)$, discount=$1$, step size=$0.01$, reward function=constant,
    space=nested.
  }\label{fig:trpo-tuning}
\end{figure}

We then train a dynamically self-learned mini Hanabi policy using each
algorithm for 1000 training iterations. Histograms of the policies' scores on
1000 games of mini Hanabi are shown in \figref{algos}.

The CMA-ES performs very poorly with an average score of 1.757.  It performs
only marginally better than a completely random policy which obtains an average
score of 1.56.
%
The CEM policy performs slightly better (but still poorly) with an average
score of 2.215.
%
The TRPO policy performs significantly better than either of the other two
policies. Most games result in a score of 8, and a significant number of games
result in a perfect score.
%
We conduct the rest of our experiments with TRPO.

\begin{figure}[ht]
  \newcommand{\algosubfig}[3]{%
    \begin{subfigure}[b]{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{data/sweep2/#1.pdf}
      \caption{#2}\label{fig:#3}
    \end{subfigure}
  }

  \centering
  \algosubfig{CMA-ES}{CMA-ES}{cmaes}
  \algosubfig{CEM}{CEM}{cem}
  \algosubfig{TRPO}{TRPO}{trpo}
  \caption{Mini Hanabi scores on 1000 mini Hanabi games.}\label{fig:algos}
\end{figure}

\subsection{Self Learning vs Guided Learning}
\todo{Generate real data.}
\todo{Comment on findings.}

\begin{figure}[ht]
  \newcommand{\bestsubfig}[1]{%
    \begin{subfigure}[b]{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{data/tmp/#1.pdf}
    \end{subfigure}
  }

  \centering

  \makebox[\textwidth][c]{%
    \begin{tabular}{|c|c|c|c|}
      \cline{2-4}
      \multicolumn{1}{c}{} &
      \multicolumn{1}{|c|}{\textbf{Mini Hanabi}} &
      \textbf{Medium Hanabi} &
      \textbf{Hanabi} \\\hline
      %
      \rotatebox[origin=l]{90}{\textbf{Dynamic Self-Learning}} &
      \bestsubfig{mini_Hanabi_dynamic_self-learned} &
      \bestsubfig{medium_Hanabi_dynamic_self-learned} &
      \bestsubfig{Hanabi_dynamic_self-learned} \\\hline
      %
      \rotatebox[origin=l]{90}{\textbf{Guided-Learning}} &
      \bestsubfig{mini_Hanabi_guided-learned} &
      \bestsubfig{medium_Hanabi_guided-learned} &
      \bestsubfig{Hanabi_guided-learned} \\\hline
      %
      \rotatebox[origin=l]{90}{\textbf{Static Self-Learning}} &
      \bestsubfig{mini_Hanabi_static_self-learned} &
      \bestsubfig{medium_Hanabi_static_self-learned} &
      \bestsubfig{Hanabi_static_self-learned} \\\hline
      %
      \rotatebox[origin=l]{90}{\textbf{Heuristic}} &
      \bestsubfig{mini_Hanabi_heuristic} &
      \bestsubfig{medium_Hanabi_heuristic} &
      \bestsubfig{Hanabi_heuristic} \\\hline
    \end{tabular}
  }

  \caption{}\label{fig:best}
\end{figure}

\subsection{Policy Behavior}
\todo{Write.}
% Describe the behavior of various policies and show that the self-learnt one
% behaves differently than the guided learnt one.
