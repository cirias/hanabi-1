\section{Approach}
\label{approach}

Here, we describe the various components we implemented to play and learn
Hanabi. Section \ref{eval} compares the performance of the different
state representations, reward functions, algorithms, and training
methods.

\subsection{Overview}
Michael

Talk about how the following pieces fit together + OpenAI


Stephanie 
\subsubsection{Space reps and Reward Functions}

%One limitation of our game engine is that our information tracking
%implementation does not keep information about which colors/numbers a card
%does not map to. For example, in the situation where ``I was told two cards
%are 3s, so the other cards are not 3s,'' our game engine does not remember that
%the other cards are not 3s.


1) Nested

2) Flattened

Talk about difficulty in fixed state rep/action mapping

\subsection{Training}

Make algs/heuristics subsections here, talk about different combos

\subsubsection{Algorithms}

\subsubsection{TRPO}

\cite{TRPO}

\subsubsection{VPG (if space)}
\subsubsection{CEM (if space)}
\subsubsection{CMA-ES (if space)}

\subsection{Heuristics}

We implemented hardcoded heuristic-based Hanabi players to see if we could
guide the previously discussed algorithms into learning a particular strategy.
Below we discuss two of these heuristic-based players.

\subsubsection{Heuristic}

Stephanie

\subsubsection{SimpleHeuristic}

Sagar

The SimpleHeuristic player attempts to mimic the strategy that our best
TRPO-trained-on-itself player learns for mini-hanabi.

