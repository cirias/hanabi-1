\section{Background [Sagar+Michael]}\label{sec:background}

\subsection{Hanabi [Sagar]}
Actually hanabi here

Hanabi \cite{hanabiboardgame, hanabiwiki} is a cooperative card game in which two or more players act on partial information towards a common goal. Each player can see the cards of all other players, but not their own. On each turn, a player can play a card (without looking at it), discard a card, or give information to another player. The goal is to play as many cards as possible in a specified order.

Figure \ref{fig:hanabisizes} shows 3 sizes of hanabi that we use in our experiments.

\begin{figure}
    \centering
    \begin{tabular}{|c | c | c | c | c | c | c |} \hline
        Name        & Colors & 1s, 2s ... 5s  & Info Tokens & Fuses & Hand Size \\ \hline
        Mini        & 3      & [2, 2, 1, 0, 0]    & 6           & 3     & 3 \\ \hline
        Medium      & 4      & [3, 2, 2, 1, 0] & 8           & 4     & 4 \\ \hline
        Regular     & 5      & [3, 2, 2, 2, 1] & 8 & 4 & 5 \\ \hline
    \end{tabular}
    \caption{Hanabi Game Sizes}
    \label{fig:hanabisizes}
\end{figure}

\subsection{Reinforcement Learning}
Reinforcement learning is a branch of machine learning distinct from supervised
learning and unsupervised learning. While supervised learning algorithms take
in labelled data and output a classifier and unsupervised learning algorithms
take in unlabeled data and output something like a clustering or
dimensionality reduction, reinforcement learning algorithms take in a
specification of an \emph{interactive environment} and output a \emph{policy}
that can optimally interact with the environment. For example, given a
specification of the board game Go or a specification of an Atari 2600,
reinforcement learning can be used to generate a policy to play Go or a policy
to play Atari 2600 games~\cite{silver2016mastering,mnih2015human}.

% Formalize MDPs.
Formally, we model an environment as a \emph{Markov decision procedure} (MDP)
which is a four-tuple $(S, A, P, R)$ where
\begin{itemize}
  \item
    $S$ is a finite set of \emph{states} (or observations) known as the
    \emph{state space} (or observation space),
  \item
    $A$ is a finite set of \emph{actions} known as the action space,
  \item
    $P: S \times A \times S \to [0, 1]$ is a \emph{transition function} which
    specifies the likelihood of transitioning from one state to another given
    an action, and
  \item
    $R: S \times A \to \reals$ is a \emph{reward function}.
\end{itemize}
A reinforcement learning algorithm learns a policy $\pi: S \to A$ which decides
which action to take in a given state. Reinforcement learning algorithms try to
learn a policy $\pi$ that maximizes the expected reward as specified by $R$.
