\section{Background}\label{sec:background}

\subsection{Hanabi}\label{sec:background:hanabi}
A Hanabi deck consists of cards labelled with one of five numbers and one of
five colors. For each of the five colors, the deck contains three 1's, two 2's,
two 3's, two 4's, and one 5.  Each player begins the game by drawing five cards
into their hand. Unlike most card games however, players \emph{cannot} see
their own cards, but they \emph{can} see all other players' cards.

The collective goal of the game is to build five stacks of cards where each
stack contains cards of a certain color. For each colored stack, a 1 must be
played first, followed by a 2, then a 3, and so on. If a player elects to play
a card that cannot be played---for example, a blue 3 when there is already a
blue 3 that has been played---one of four \emph{fuses} is used up. If the group
runs out of these fuses, the game ends.

In addition to playing cards, players can inform other players about the cards
in their hand. A player can either point out all of the cards of a given number
(e.g. ``these three cards are 2's'') or point out all of the cards of a given
color (e.g. ``these two cards are red''). When a player gives information, one
of eight \emph{information tokens} is used. Information can only be given if
there are information tokens remaining. Players may also discard a card to
re-gain one information token.

The game ends either when all fuses are used up, all cards are played (a
perfect game), or when the deck is empty and all players have played one
additional move after the last card was drawn. The final score is the number of
played cards.

In addition to the standard version of Hanabi described here, Hanabi can also
be adjusted to different sizes by changing the number of colors, numbers,
information tokens, fuses, and hand size. We discuss this further in
\secref{eval:hanabivariants}.  Furthermore, in this work, we focus on
two-player Hanabi.

\subsection{Markov Decision Processes}\label{sec:background:mdp}
Markov decision processes (MDPs) are stochastic state machines~\cite{mdp}. Given
a state and an action, the next state is chosen based on some probability
distribution. Each state transition is also associated with some immediate
reward.

Formally, we model an environment as a \emph{Markov decision process} (MDP)
which is a four-tuple $(S, A, P, R)$ where
\begin{itemize}
  \setlength\itemsep{0em}
  \item
    $S$ is a finite set of \emph{states} (or observations) known as the
    \emph{state space} (or observation space),
  \item
    $A$ is a finite set of \emph{actions} known as the action space,
  \item
    $P: S \times A \times S \to [0, 1]$ is a \emph{transition function} which
    specifies the likelihood of transitioning from one state to another given
    an action, and
  \item
    $R: S \times A \to \reals$ is a \emph{reward function}.
\end{itemize}

The goal in MDPs is to learn a policy $\pi : S \to A$ that decides which action
to take in a given state in order to maximize the expected reward, as specified
by $R$.

Reinforcement learning is a branch of machine learning distinct from supervised
learning and unsupervised learning. Reinforcement learning includes a family of
methods that can be used to solve MDPs in which the underlying probability
distribution is unknown~\cite{sutton1998reinforcement}.  Reinforcement learning
has been used to play a wide variety of interactive games including
Go~\cite{silver2016mastering} and Atari 2600 video games~\cite{mnih2015human}.
