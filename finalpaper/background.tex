\section{Background [Sagar+Michael]}
\label{sec:background}

\subsection{Hanabi [Sagar]}
\label{sec:background:hanabi}

Hanabi \cite{hanabiboardgame, hanabiwiki} is a cooperative card game in which
two or more players act on partial information towards a common goal. The
card deck consists of cards labelled with a number, color pair. In regular
Hanabi, there are five possible colors and five possible numbers, one to five.
Unlike a traditional card deck, the deck contains duplicates of certain cards;
as shown in the final row of Figure~\ref{fig:hanabisizes}, a regular Hanabi
deck contains three cards numbered one, two cards numbered two, and so on, for
each of the five colors. Each player begins the game with a hand containing
five cards. Unlike most card games however, players \emph{cannot} see their
own cards, but \emph{can} see all other player's cards.

The collective goal of the game is to play cards on the table such that a stack
is completed for each color with numbers 1 to 5. Per color, cards must be
played in a strictly-increasing-by-one order. That is, for any particular
color, a one can be played only when no cards of that color have already been
played, a two can only be played when only a one of that color has been played,
and so on.  If a player elects to play a card that cannot be played - for
example a blue three when there is already a blue three that has been played
- one of four ``fuses'' is used up. If the group runs out of these fuses, the
game ends.

In addition to playing cards, players can inform other players about cards
in their hand. An inform move involves one player providing one other player
with information about their hand. This information can contain either number
of color information, but not both. Furthermore, this information must be complete;
the player must tell the other player about all cards of the particular number or
color they are informing about. When a player makes an inform move, one info
token is used. In regular Hanabi, the group begins with eight info tokens.

The final type of move that a player may move is to discard a card. When a card
is discarded, one info token is returned to the group.

The game ends either when all fuses are used up, all cards are played (a perfect
game), or when the deck is empty and all players have played one additional
move after the last card was drawn. The final score can be computed by summing
the top value on each stack, with a higher score being more desireable. In addition
to the regular version described here, Hanabi can also be adjusted to different
sizes by changing the number of colors, numbers, information tokens, fuses, and 
hand size. Figure~\ref{fig:hanabisizes} shows 3 sizes of hanabi that we use in
our experiments.

\begin{figure}[ht]
    \centering
    \begin{tabular}{|c | c | c | c | c | c | c |} \hline
        Name        & Colors & 1s, 2s ... 5s  & Info Tokens & Fuses & Hand Size \\ \hline
        Mini        & 3      & [2, 2, 1, 0, 0]    & 6           & 3     & 3 \\ \hline
        Medium      & 4      & [3, 2, 2, 1, 0] & 8           & 4     & 4 \\ \hline
        Regular     & 5      & [3, 2, 2, 2, 1] & 8 & 4 & 5 \\ \hline
    \end{tabular}
    \caption{Hanabi Game Sizes}
    \label{fig:hanabisizes}
\end{figure}


% Things to talk about:
% TODO, maybe: -game difficult vs size/number of cards that must be played
% -human results here?



\subsection{Reinforcement Learning}
\label{sec:background:mdp}
\todo{Let's make this about MDPs in general and then discuss RL in particular}
Reinforcement learning is a branch of machine learning distinct from supervised
learning and unsupervised learning. While supervised learning algorithms take
in labelled data and output a classifier and unsupervised learning algorithms
take in unlabeled data and output something like a clustering or
dimensionality reduction, reinforcement learning algorithms take in a
specification of an \emph{interactive environment} and output a \emph{policy}
that can optimally interact with the environment. For example, given a
specification of the board game Go or a specification of an Atari 2600,
reinforcement learning can be used to generate a policy to play Go or a policy
to play Atari 2600 games~\cite{silver2016mastering,mnih2015human}.

% Formalize MDPs.
Formally, we model an environment as a \emph{Markov decision process} (MDP)
which is a four-tuple $(S, A, P, R)$ where
\begin{itemize}
  \setlength\itemsep{0em}
  \item
    $S$ is a finite set of \emph{states} (or observations) known as the
    \emph{state space} (or observation space),
  \item
    $A$ is a finite set of \emph{actions} known as the action space,
  \item
    $P: S \times A \times S \to [0, 1]$ is a \emph{transition function} which
    specifies the likelihood of transitioning from one state to another given
    an action, and
  \item
    $R: S \times A \to \reals$ is a \emph{reward function}.
\end{itemize}
A reinforcement learning algorithm learns a policy $\pi: S \to A$ which decides
which action to take in a given state. Reinforcement learning algorithms try to
learn a policy $\pi$ that maximizes the expected reward as specified by $R$.
